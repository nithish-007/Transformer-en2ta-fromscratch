{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "559c8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# importing Libraries\n",
    "# ---------------------------------\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "476678f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Input Embeddings\n",
    "# ---------------------------------\n",
    "\n",
    "class InputEnbeddings(nn.Module):\n",
    "    def __init__(self, d_model:int, vocab_size:int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forwad(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the the paper\n",
    "        return self.embeddings(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eb0276f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 9, 8, 1, 6],\n",
       "         [2, 3, 3, 7, 6]]),\n",
       " Parameter containing:\n",
       " tensor([[-1.2780,  0.8751, -0.2351, -1.2761,  0.5549],\n",
       "         [ 0.1548,  0.1161,  0.9773,  1.2093, -0.3735],\n",
       "         [-0.2943, -2.1374, -0.1426, -0.6374,  0.8016],\n",
       "         [-2.4155,  0.0477, -2.1817,  0.6429, -0.0706],\n",
       "         [-0.1482,  0.0647, -0.0669, -0.3310, -0.7109],\n",
       "         [ 0.2538,  1.6435,  0.5717, -0.1346, -0.7974],\n",
       "         [ 0.8568, -0.6566,  1.0557, -1.2519,  0.2087],\n",
       "         [ 1.2182, -0.4036, -0.4436,  0.5534, -1.1589],\n",
       "         [-1.4049, -2.0428,  3.1682, -0.5440,  1.0847],\n",
       "         [-1.0299,  0.8130, -0.3291,  1.2895,  0.8362]], requires_grad=True),\n",
       " tensor([[[ 0.1548,  0.1161,  0.9773,  1.2093, -0.3735],\n",
       "          [-1.0299,  0.8130, -0.3291,  1.2895,  0.8362],\n",
       "          [-1.4049, -2.0428,  3.1682, -0.5440,  1.0847],\n",
       "          [ 0.1548,  0.1161,  0.9773,  1.2093, -0.3735],\n",
       "          [ 0.8568, -0.6566,  1.0557, -1.2519,  0.2087]],\n",
       " \n",
       "         [[-0.2943, -2.1374, -0.1426, -0.6374,  0.8016],\n",
       "          [-2.4155,  0.0477, -2.1817,  0.6429, -0.0706],\n",
       "          [-2.4155,  0.0477, -2.1817,  0.6429, -0.0706],\n",
       "          [ 1.2182, -0.4036, -0.4436,  0.5534, -1.1589],\n",
       "          [ 0.8568, -0.6566,  1.0557, -1.2519,  0.2087]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Embedding(10, 5); b = torch.randint(low=1, high=10, size=(2,5))\n",
    "b, a.weight, a(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5919dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Positional Embeddings\n",
    "# ---------------------------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model:int, seq_len:int, dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len, 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) #(d_model / 2)\n",
    "        # apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position / 10000**(2i / d_model))\n",
    "        # apply cosine to off indices \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position / 10000**(2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding \n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c2bdec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1548,  0.1161,  0.9773,  1.2093, -0.3735],\n",
       "         [-1.0299,  0.8130, -0.3291,  1.2895,  0.8362],\n",
       "         [-1.4049, -2.0428,  3.1682, -0.5440,  1.0847],\n",
       "         [ 0.1548,  0.1161,  0.9773,  1.2093, -0.3735],\n",
       "         [ 0.8568, -0.6566,  1.0557, -1.2519,  0.2087]],\n",
       "\n",
       "        [[-0.2943, -2.1374, -0.1426, -0.6374,  0.8016],\n",
       "         [-2.4155,  0.0477, -2.1817,  0.6429, -0.0706],\n",
       "         [-2.4155,  0.0477, -2.1817,  0.6429, -0.0706],\n",
       "         [ 1.2182, -0.4036, -0.4436,  0.5534, -1.1589],\n",
       "         [ 0.8568, -0.6566,  1.0557, -1.2519,  0.2087]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb7c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# MultiHead Attention\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb51de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Encoder Block\n",
    "# ---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Residual Connection\n",
    "# ---------------------------------\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8422aecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2, 7, 1],\n",
       "         [6, 3, 5],\n",
       "         [1, 9, 3]]),\n",
       " tensor([[2],\n",
       "         [9],\n",
       "         [6]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "a = torch.randint(low=1, high=10, size=(3,3)); b = torch.randint(low=1, high=10, size=(3,1))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee90cde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 7.4989e-01, 5.6234e-01, 4.2170e-01, 3.1623e-01, 2.3714e-01,\n",
       "        1.7783e-01, 1.3335e-01, 1.0000e-01, 7.4989e-02, 5.6234e-02, 4.2170e-02,\n",
       "        3.1623e-02, 2.3714e-02, 1.7783e-02, 1.3335e-02, 1.0000e-02, 7.4989e-03,\n",
       "        5.6234e-03, 4.2170e-03, 3.1623e-03, 2.3714e-03, 1.7783e-03, 1.3335e-03,\n",
       "        1.0000e-03, 7.4989e-04, 5.6234e-04, 4.2170e-04, 3.1623e-04, 2.3714e-04,\n",
       "        1.7783e-04, 1.3335e-04])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import math \n",
    "\n",
    "d_model = 64\n",
    "position = torch.arange(0, 10).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) #(d_model / 2)\n",
    "div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6e1a325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.0000e+00, 7.4989e-01, 5.6234e-01, 4.2170e-01, 3.1623e-01, 2.3714e-01,\n",
       "         1.7783e-01, 1.3335e-01, 1.0000e-01, 7.4989e-02, 5.6234e-02, 4.2170e-02,\n",
       "         3.1623e-02, 2.3714e-02, 1.7783e-02, 1.3335e-02, 1.0000e-02, 7.4989e-03,\n",
       "         5.6234e-03, 4.2170e-03, 3.1623e-03, 2.3714e-03, 1.7783e-03, 1.3335e-03,\n",
       "         1.0000e-03, 7.4989e-04, 5.6234e-04, 4.2170e-04, 3.1623e-04, 2.3714e-04,\n",
       "         1.7783e-04, 1.3335e-04],\n",
       "        [2.0000e+00, 1.4998e+00, 1.1247e+00, 8.4339e-01, 6.3246e-01, 4.7427e-01,\n",
       "         3.5566e-01, 2.6670e-01, 2.0000e-01, 1.4998e-01, 1.1247e-01, 8.4339e-02,\n",
       "         6.3246e-02, 4.7427e-02, 3.5566e-02, 2.6670e-02, 2.0000e-02, 1.4998e-02,\n",
       "         1.1247e-02, 8.4339e-03, 6.3246e-03, 4.7427e-03, 3.5566e-03, 2.6670e-03,\n",
       "         2.0000e-03, 1.4998e-03, 1.1247e-03, 8.4339e-04, 6.3246e-04, 4.7427e-04,\n",
       "         3.5566e-04, 2.6670e-04],\n",
       "        [3.0000e+00, 2.2497e+00, 1.6870e+00, 1.2651e+00, 9.4868e-01, 7.1141e-01,\n",
       "         5.3348e-01, 4.0006e-01, 3.0000e-01, 2.2497e-01, 1.6870e-01, 1.2651e-01,\n",
       "         9.4868e-02, 7.1141e-02, 5.3348e-02, 4.0006e-02, 3.0000e-02, 2.2497e-02,\n",
       "         1.6870e-02, 1.2651e-02, 9.4868e-03, 7.1141e-03, 5.3348e-03, 4.0006e-03,\n",
       "         3.0000e-03, 2.2497e-03, 1.6870e-03, 1.2651e-03, 9.4868e-04, 7.1141e-04,\n",
       "         5.3348e-04, 4.0006e-04],\n",
       "        [4.0000e+00, 2.9996e+00, 2.2494e+00, 1.6868e+00, 1.2649e+00, 9.4855e-01,\n",
       "         7.1131e-01, 5.3341e-01, 4.0000e-01, 2.9996e-01, 2.2494e-01, 1.6868e-01,\n",
       "         1.2649e-01, 9.4855e-02, 7.1131e-02, 5.3341e-02, 4.0000e-02, 2.9996e-02,\n",
       "         2.2494e-02, 1.6868e-02, 1.2649e-02, 9.4855e-03, 7.1131e-03, 5.3341e-03,\n",
       "         4.0000e-03, 2.9996e-03, 2.2494e-03, 1.6868e-03, 1.2649e-03, 9.4855e-04,\n",
       "         7.1131e-04, 5.3341e-04],\n",
       "        [5.0000e+00, 3.7495e+00, 2.8117e+00, 2.1085e+00, 1.5811e+00, 1.1857e+00,\n",
       "         8.8914e-01, 6.6676e-01, 5.0000e-01, 3.7495e-01, 2.8117e-01, 2.1085e-01,\n",
       "         1.5811e-01, 1.1857e-01, 8.8914e-02, 6.6676e-02, 5.0000e-02, 3.7495e-02,\n",
       "         2.8117e-02, 2.1085e-02, 1.5811e-02, 1.1857e-02, 8.8914e-03, 6.6676e-03,\n",
       "         5.0000e-03, 3.7495e-03, 2.8117e-03, 2.1085e-03, 1.5811e-03, 1.1857e-03,\n",
       "         8.8914e-04, 6.6676e-04],\n",
       "        [6.0000e+00, 4.4994e+00, 3.3740e+00, 2.5302e+00, 1.8974e+00, 1.4228e+00,\n",
       "         1.0670e+00, 8.0011e-01, 6.0000e-01, 4.4994e-01, 3.3740e-01, 2.5302e-01,\n",
       "         1.8974e-01, 1.4228e-01, 1.0670e-01, 8.0011e-02, 6.0000e-02, 4.4994e-02,\n",
       "         3.3740e-02, 2.5302e-02, 1.8974e-02, 1.4228e-02, 1.0670e-02, 8.0011e-03,\n",
       "         6.0000e-03, 4.4994e-03, 3.3740e-03, 2.5302e-03, 1.8974e-03, 1.4228e-03,\n",
       "         1.0670e-03, 8.0011e-04],\n",
       "        [7.0000e+00, 5.2493e+00, 3.9364e+00, 2.9519e+00, 2.2136e+00, 1.6600e+00,\n",
       "         1.2448e+00, 9.3347e-01, 7.0000e-01, 5.2493e-01, 3.9364e-01, 2.9519e-01,\n",
       "         2.2136e-01, 1.6600e-01, 1.2448e-01, 9.3346e-02, 7.0000e-02, 5.2493e-02,\n",
       "         3.9364e-02, 2.9519e-02, 2.2136e-02, 1.6600e-02, 1.2448e-02, 9.3346e-03,\n",
       "         7.0000e-03, 5.2493e-03, 3.9364e-03, 2.9519e-03, 2.2136e-03, 1.6600e-03,\n",
       "         1.2448e-03, 9.3346e-04],\n",
       "        [8.0000e+00, 5.9992e+00, 4.4987e+00, 3.3736e+00, 2.5298e+00, 1.8971e+00,\n",
       "         1.4226e+00, 1.0668e+00, 8.0000e-01, 5.9992e-01, 4.4987e-01, 3.3736e-01,\n",
       "         2.5298e-01, 1.8971e-01, 1.4226e-01, 1.0668e-01, 8.0000e-02, 5.9992e-02,\n",
       "         4.4987e-02, 3.3736e-02, 2.5298e-02, 1.8971e-02, 1.4226e-02, 1.0668e-02,\n",
       "         8.0000e-03, 5.9992e-03, 4.4987e-03, 3.3736e-03, 2.5298e-03, 1.8971e-03,\n",
       "         1.4226e-03, 1.0668e-03],\n",
       "        [9.0000e+00, 6.7490e+00, 5.0611e+00, 3.7953e+00, 2.8460e+00, 2.1342e+00,\n",
       "         1.6005e+00, 1.2002e+00, 9.0000e-01, 6.7490e-01, 5.0611e-01, 3.7953e-01,\n",
       "         2.8460e-01, 2.1342e-01, 1.6005e-01, 1.2002e-01, 9.0000e-02, 6.7490e-02,\n",
       "         5.0611e-02, 3.7953e-02, 2.8460e-02, 2.1342e-02, 1.6005e-02, 1.2002e-02,\n",
       "         9.0000e-03, 6.7490e-03, 5.0611e-03, 3.7953e-03, 2.8461e-03, 2.1342e-03,\n",
       "         1.6005e-03, 1.2002e-03]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position * div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d76386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
